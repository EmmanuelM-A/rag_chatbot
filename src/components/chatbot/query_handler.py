"""
Responsible for handling user queries and generating their corresponding
response.
"""

from langchain_openai import OpenAIEmbeddings
import numpy as np
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

from src.components.config.config import DEFAULT_RESPONSE_PROMPT_FILEPATH
from src.components.prompts.prompt_loader import create_prompt_template
from src.utils.logger import get_logger
from config import (RETRIEVAL_TOP_K, LLM_TEMPERATURE)

logger = get_logger(__name__)


class QueryHandler:
    """
    Handles the query processing and the response generation.
    """

    def __init__(self, embedding_model_name: str, llm_model_name: str) -> None:
        self.embedding_model_name = embedding_model_name
        self.llm_model_name = llm_model_name

    def search(self, query: str, index, metadata):
        """
        Embeds query, searches vector DB, returns top_k results.
        """

        embedding_model = OpenAIEmbeddings(model=self.embedding_model_name)

        query_vector = embedding_model.embed_query(query)

        _, I = index.search(np.array([query_vector]).astype("float32"),
                            RETRIEVAL_TOP_K)

        results = []

        for i in I[0]:
            entry = metadata[i]
            results.append({
                "text": entry["text"],
                "metadata": entry["meta"]
            })

        if not results:
            logger.error(f"No results found for the query: {query}")
            return None

        logger.info("Query embedded and results found.")

        return results

    def generate_responses(self, query: str, retrieved_chunks):
        """
        Formats a prompt with the query and retrieved chunks, then passes it
        to an LLM.
        """

        logger.debug(f"Generating responses for the query: {query}")

        llm = ChatOpenAI(
            model=self.llm_model_name,
            temperature=LLM_TEMPERATURE
        )

        context_text = "\n\n".join(
            [chunk["text"] for chunk in retrieved_chunks])

        prompt_template = create_prompt_template(
            DEFAULT_RESPONSE_PROMPT_FILEPATH)

        # Create a chain for processing
        rag_chain = prompt_template | llm | StrOutputParser()

        response = rag_chain.invoke({"context": context_text, "query": query})

        # Extract unique sources from the retrieved chunks
        sources = list(
            set([chunk["metadata"]["source"] for chunk in retrieved_chunks if
                 "source" in chunk["metadata"]]))

        logger.info("Response generated by LLM.")

        return {"answer": response, "sources": sources}
