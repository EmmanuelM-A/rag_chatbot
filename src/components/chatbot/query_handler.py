"""
Responsible for handling user queries and generating their corresponding
response.
"""

from langchain_openai import OpenAIEmbeddings
import numpy as np
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

from src.components.config.settings import settings
from src.components.prompts.prompt_loader import create_prompt_template
from src.components.config.logger import get_logger

logger = get_logger(__name__)


class QueryHandler:
    """
    Handles the query processing and the response generation.
    """

    def __init__(self, embedding_model_name: str, llm_model_name: str) -> None:
        self.embedding_model = OpenAIEmbeddings(model=embedding_model_name)
        self.llm_model_name = llm_model_name

    def search(self, query: str, index, metadata):
        """
        Embeds query, searches vector DB, returns top_k results.
        """

        query_vector = self.embedding_model.embed_query(query)

        _, I = index.search(np.array([query_vector]).astype("float32"),
                            settings.RETRIEVAL_TOP_K)

        results = []

        for i in I[0]:
            entry = metadata[i]
            results.append({
                "text": entry["text"],
                "metadata": entry["meta"]
            })

        if not results:
            logger.error(f"No results found for the query: {query}")
            return None

        logger.info("Query embedded and results found.")

        return results

    def generate_responses(self, query: str, retrieved_chunks):
        """
        Formats a prompt with the query and retrieved chunks, then passes it
        to an LLM.
        """

        logger.debug(f"Generating responses for the query: {query}")

        llm = ChatOpenAI(
            model=self.llm_model_name,
            temperature=settings.LLM_TEMPERATURE
        )

        context_text = "\n\n".join(
            [chunk["text"] for chunk in retrieved_chunks])

        prompt_template = create_prompt_template(
            settings.RESPONSE_PROMPT_FILEPATH)

        # Create a chain for processing
        rag_chain = prompt_template | llm | StrOutputParser()

        response = rag_chain.invoke({"context": context_text, "query": query})

        if response.strip() == "NEED_WEB_SEARCH":
            return None

        # Extract unique sources with error handling
        sources = []
        for chunk in retrieved_chunks:
            try:
                metadata = chunk["metadata"]
                if hasattr(metadata, "source") and metadata.source:
                    sources.append(metadata.source)
            except (KeyError, AttributeError) as e:
                logger.warning(
                    f"Could not extract source from chunk metadata: {e}")
                continue

        # Remove duplicates
        sources = list(set(sources))

        logger.info("Response generated by LLM.")

        return {"answer": response, "sources": sources}
