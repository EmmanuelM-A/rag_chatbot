from langchain_openai import OpenAIEmbeddings
import numpy as np
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from utils.logger import get_logger
from config import EMBEDDING_MODEL_NAME, RETRIEVAL_TOP_K, LLM_MODEL_NAME, LLM_TEMPERATURE

logger = get_logger("query_handler_logger")


def search(query, index, metadata, embedding_model=None):
    """
    Embeds query, searches vector DB, returns top_k results.
    """

    if embedding_model is None:
        logger.warning(f"The default embedding model: {EMBEDDING_MODEL_NAME} has been set!")
        embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)

    query_vector = embedding_model.embed_query(query)

    D, I = index.search(np.array([query_vector]).astype("float32"), RETRIEVAL_TOP_K)

    results = []

    for i in I[0]:
        entry = metadata[i]
        results.append({
            "text": entry["text"],
            "metadata": entry["meta"]
        })

    logger.info("Query embedded and results found.")

    return results


def generate_response(query, retrieved_chunks, llm=None):
    """
    Formats a prompt with the query and retrieved chunks, then passes it to an LLM.
    """

    if llm is None:
        logger.warning(f"The default LLM: {LLM_MODEL_NAME} has been set!")
        llm = ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=LLM_TEMPERATURE)

    context_text = "\n\n".join([chunk["text"] for chunk in retrieved_chunks])

    # Define your prompt template
    # It's crucial to instruct the LLM to use only the provided context.
    prompt_template = ChatPromptTemplate.from_messages(
        [
            ("system", "You are a helpful AI assistant. Answer the user's question based *only* on the provided context. If you "
             "cannot find the answer in the context, politely state that you don't have enough information."),
            ("user", "Context: {context}\n\nQuestion: {query}"),
        ]
    )

    # Create a chain for processing
    rag_chain = prompt_template | llm | StrOutputParser()

    response = rag_chain.invoke({"context": context_text, "query": query})

    # Extract unique sources from the retrieved chunks
    sources = list(set([chunk["metadata"]["source"] for chunk in retrieved_chunks if "source" in chunk["metadata"]]))

    logger.info("Response generated by LLM.")

    return {"answer": response, "sources": sources}