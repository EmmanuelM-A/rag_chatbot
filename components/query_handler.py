from langchain_openai import OpenAIEmbeddings
import numpy as np
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

from components.config import DEFAULT_RESPONSE_PROMPT_FILEPATH
from components.prompt_loader import create_prompt_template
from utils.logger import get_logger
from config import EMBEDDING_MODEL_NAME, RETRIEVAL_TOP_K, LLM_MODEL_NAME, LLM_TEMPERATURE

logger = get_logger("query_handler_logger")


def search(query, index, metadata, embedding_model=None):
    """
    Embeds query, searches vector DB, returns top_k results.
    """

    if embedding_model is None:
        logger.warning(f"The default embedding model: {EMBEDDING_MODEL_NAME} has been set!")
        embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)

    query_vector = embedding_model.embed_query(query)

    D, I = index.search(np.array([query_vector]).astype("float32"), RETRIEVAL_TOP_K)

    results = []

    for i in I[0]:
        entry = metadata[i]
        results.append({
            "text": entry["text"],
            "metadata": entry["meta"]
        })

    if results:
        logger.info("Query embedded and results found.")
    else:
        logger.error(f"No results found for the query: {query}")

    return results


def generate_response(query, retrieved_chunks, llm=None):
    """
    Formats a prompt with the query and retrieved chunks, then passes it to an LLM.
    """

    logger.debug(f"Generating responses for the query: {query}")

    if llm is None:
        logger.warning(f"The default LLM: {LLM_MODEL_NAME} has been set!")
        llm = ChatOpenAI(model_name=LLM_MODEL_NAME, temperature=LLM_TEMPERATURE)

    context_text = "\n\n".join([chunk["text"] for chunk in retrieved_chunks])

    # Define your prompt template
    prompt_template = create_prompt_template(DEFAULT_RESPONSE_PROMPT_FILEPATH)

    # Create a chain for processing
    rag_chain = prompt_template | llm | StrOutputParser()

    response = rag_chain.invoke({"context": context_text, "query": query})

    # Extract unique sources from the retrieved chunks
    sources = list(set([chunk["metadata"]["source"] for chunk in retrieved_chunks if "source" in chunk["metadata"]]))

    logger.info("Response generated by LLM.")

    return {"answer": response, "sources": sources}